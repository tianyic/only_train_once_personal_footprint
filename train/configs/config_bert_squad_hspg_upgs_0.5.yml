backend: bert
model_name_or_path: bert-base-uncased
train_file: data/squad/train-v1.1.json
eval_file: data/squad/dev-v1.1.json
ckpt_initial: ~
load_embedding_only: false

do_train: true
do_eval: true
do_lower_case: true

max_seq_length: 384
doc_stride: 128
max_query_length: 64
overwrite_output_dir: true
checkpoint_dir: checkpoints/bert_training
seed: !!int 0

version_2_with_negative: false

# Training option
train:
    batch_size: 32
    log_ckpt_freq: !!int 200 # per steps
    max_epoch: 9
    gradient_accumulation_steps: 1
    evaluate_during_training: true
    evaluate_ckpt_freq: !!int 20 # per log_ckpt_freq
    save_ckpt_freq: !!int 20 # per log_ckpt_freq
    max_steps: -1
    train_times: !!int 2

# Evalution option
eval:
    batch_size: 3
    n_best_size: 20
    max_answer_length: 30
    verbose_logging: false

optimizer:
    name: hspg
    init_stage: sgd
    init_lr: 0.1
    n_p: 1
    lambda: !!float 1e-3
    decay_mode: multi_step
    decay_lr_epochs: [4]
    decay_lambda_epochs: [2, 4, 6]
    momentum: 0.0
    max_grad_norm: 1.0
    epsilon: [0.0, 0.0, 0.0, 0.0, 0.0]
    adapt_epsilon_freq: !!int 200 # per steps
    adapt_epsilon: [false, false, false, true, false]
    upper_group_sparsity: [1.0, 1.0, 1.0, 0.5, 1.0]
    weight_decay: 0.0001

lr_scheduler:
    warmup_steps: 0